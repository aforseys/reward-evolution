{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e0c1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reward_machines\n",
    "#import reward_machines.envs\n",
    "import gymnasium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c6028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_env(env_id, wrapper_kwargs=None, env_kwargs=None):\n",
    "#     wrapper_kwargs = wrapper_kwargs or {}\n",
    "#     env_kwargs = env_kwargs or {}\n",
    "#     env = gymnasium.make(env_id, wrapper_kwargs, env_kwargs)\n",
    "#make_env(\"Obstacle-v0\") #figure out how to get this working? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558027f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[['a' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['.' '.' '.' '.' 'X' '.' '.' '.']\n",
      " ['.' '.' 'C' 'X' 'X' '.' '.' '.']\n",
      " ['.' '.' '.' 'X' '.' '.' '.' '.']\n",
      " ['.' 'H' '.' '.' '.' '.' '.' '.']\n",
      " ['.' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['.' '.' '.' '.' 'B' '.' '.' '.']\n",
      " ['.' '.' '.' '.' '.' '.' '.' '.']]\n"
     ]
    }
   ],
   "source": [
    "from reward_machines.envs.grids.grid_world import *\n",
    "from reward_machines.envs.grids.grid_world_map import GridWorldMap\n",
    "\n",
    "rm_files = [\"./reward_machines/envs/grids/reward_machines/rm2_one_feature.txt\"]\n",
    "map_file = \"./reward_machines/envs/grids/maps/map1.txt\"\n",
    "\n",
    "test_env = ObstacleRMEnv(rm_files, map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f0be1b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dict('features': Dict('a_loc': Box(0, 7, (2,), int64), 'c_loc': Box(0, 7, (2,), int64), 'b_loc': Box(0, 7, (2,), int64), 'h_loc': Box(0, 7, (2,), int64)), 'rm-state': Box(0, 1, (2,), uint8)), {'features': {'a_loc': array([0, 0]), 'c_loc': array([2, 2]), 'b_loc': array([6, 4]), 'h_loc': [array([4, 1])]}, 'rm-state': array([1., 0.])}] {'distance_B': 10.0}\n"
     ]
    }
   ],
   "source": [
    "#Reset environment places agent back at starting location (0,0)\n",
    "#and back at initial state in RM\n",
    "obs, info = test_env.reset()\n",
    "print(obs,info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9e2b60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Locations given by (row,col)\n",
    "#(0,0) index in upper left corner \n",
    "\n",
    "#Agent actions that can be taken are:  \n",
    "#0: right (+1 col)\n",
    "#1: down  (+1 row)\n",
    "#2: left  (-1 col)\n",
    "#3: up    (-1 row)\n",
    "\n",
    "#e.g.\n",
    "#test_env.step(0) #takes a step right\n",
    "\n",
    "#Taking a step out of bounds will not change the location\n",
    "#e.g.\n",
    "#test_env.step(3) #will not change location (going up sends out of bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53f98c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Dict('features': Dict('a_loc': Box(0, 7, (2,), int64), 'c_loc': Box(0, 7, (2,), int64), 'b_loc': Box(0, 7, (2,), int64), 'h_loc': Box(0, 7, (2,), int64)), 'rm-state': Box(0, 1, (2,), uint8)),\n",
       "  {'features': {'a_loc': array([2, 2]),\n",
       "    'c_loc': array([2, 2]),\n",
       "    'b_loc': array([6, 4]),\n",
       "    'h_loc': [array([4, 1])]},\n",
       "   'rm-state': array([0., 0.])}],\n",
       " 100,\n",
       " True,\n",
       " {'distance_B': 6.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test having agent travel to both targets in appropriate order \n",
    "\n",
    "#Reset environment\n",
    "test_env.reset()\n",
    "#Travel to first target \n",
    "test_env.step(0)\n",
    "test_env.step(0)\n",
    "test_env.step(0)\n",
    "test_env.step(0)\n",
    "test_env.step(0)\n",
    "test_env.step(1)\n",
    "test_env.step(1)\n",
    "test_env.step(1)\n",
    "test_env.step(1)\n",
    "test_env.step(1)\n",
    "test_env.step(1)\n",
    "test_env.step(2)\n",
    "#Travel to second target \n",
    "test_env.step(2)\n",
    "test_env.step(2)\n",
    "test_env.step(3)\n",
    "test_env.step(3)\n",
    "test_env.step(3)\n",
    "test_env.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed23ff4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "the RM was set to a terminal state!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Test that can't take another step after terminal state reached \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/reward_evolution/reward_machines/reward_machines/rm_environment.py:93\u001b[0m, in \u001b[0;36mRewardMachineEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# update the RM state\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_u_id, rm_rew, rm_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_rm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_u_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_props\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# returning the result of this action\u001b[39;00m\n\u001b[1;32m     96\u001b[0m done \u001b[38;5;241m=\u001b[39m rm_done \u001b[38;5;129;01mor\u001b[39;00m env_done\n",
      "File \u001b[0;32m~/Desktop/reward_evolution/reward_machines/reward_machines/reward_machine.py:46\u001b[0m, in \u001b[0;36mRewardMachine.step\u001b[0;34m(self, u1, true_props, s_info, env_done)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03mEmulates an step on the reward machine from state *u1* when observing *true_props*.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mThe rest of the parameters are for computing the reward when working with non-simple RMs: s_info (extra state information to compute the reward).\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Computing the next state in the RM and checking if the episode is done\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m u1 \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminal_u, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe RM was set to a terminal state!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m u2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_next_state(u1, true_props)\n\u001b[1;32m     48\u001b[0m done \u001b[38;5;241m=\u001b[39m (u2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminal_u)\n",
      "\u001b[0;31mAssertionError\u001b[0m: the RM was set to a terminal state!"
     ]
    }
   ],
   "source": [
    "#Test that can't take another step after terminal state reached \n",
    "test_env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ca68d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Dict('features': Dict('a_loc': Box(0, 7, (2,), int64), 'c_loc': Box(0, 7, (2,), int64), 'b_loc': Box(0, 7, (2,), int64), 'h_loc': Box(0, 7, (2,), int64)), 'rm-state': Box(0, 1, (2,), uint8)),\n",
       "  {'features': {'a_loc': array([4, 1]),\n",
       "    'c_loc': array([2, 2]),\n",
       "    'b_loc': array([6, 4]),\n",
       "    'h_loc': [array([4, 1])]},\n",
       "   'rm-state': array([0., 0.])}],\n",
       " -100,\n",
       " True,\n",
       " {'distance_B': 5.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test traveling to first target then hazard \n",
    "\n",
    "#Reset environment \n",
    "test_env.reset()\n",
    "#Travel to first target \n",
    "test_env.step(0)\n",
    "test_env.step(0)\n",
    "test_env.step(0)\n",
    "test_env.step(0)\n",
    "test_env.step(0)\n",
    "test_env.step(1)\n",
    "test_env.step(1)\n",
    "test_env.step(1)\n",
    "test_env.step(1)\n",
    "test_env.step(1)\n",
    "test_env.step(1)\n",
    "test_env.step(2)\n",
    "#Travel to hazard \n",
    "test_env.step(2)\n",
    "test_env.step(2)\n",
    "test_env.step(2)\n",
    "test_env.step(3)\n",
    "test_env.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "037923ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "the RM was set to a terminal state!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Test that can't take another step after terminal state reached \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/reward_evolution/reward_machines/reward_machines/rm_environment.py:93\u001b[0m, in \u001b[0;36mRewardMachineEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# update the RM state\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_u_id, rm_rew, rm_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_rm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_u_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_props\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# returning the result of this action\u001b[39;00m\n\u001b[1;32m     96\u001b[0m done \u001b[38;5;241m=\u001b[39m rm_done \u001b[38;5;129;01mor\u001b[39;00m env_done\n",
      "File \u001b[0;32m~/Desktop/reward_evolution/reward_machines/reward_machines/reward_machine.py:46\u001b[0m, in \u001b[0;36mRewardMachine.step\u001b[0;34m(self, u1, true_props, s_info, env_done)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03mEmulates an step on the reward machine from state *u1* when observing *true_props*.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mThe rest of the parameters are for computing the reward when working with non-simple RMs: s_info (extra state information to compute the reward).\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Computing the next state in the RM and checking if the episode is done\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m u1 \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminal_u, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe RM was set to a terminal state!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m u2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_next_state(u1, true_props)\n\u001b[1;32m     48\u001b[0m done \u001b[38;5;241m=\u001b[39m (u2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminal_u)\n",
      "\u001b[0;31mAssertionError\u001b[0m: the RM was set to a terminal state!"
     ]
    }
   ],
   "source": [
    "#Test that can't take another step after terminal state reached \n",
    "test_env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c7ed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Dict('features': Dict('a_loc': Box(0, 7, (2,), int64), 'c_loc': Box(0, 7, (2,), int64), 'b_loc': Box(0, 7, (2,), int64), 'h_loc': Box(0, 7, (2,), int64)), 'rm-state': Box(0, 1, (2,), uint8)),\n",
       "  {'features': {'a_loc': array([2, 2]),\n",
       "    'c_loc': array([2, 2]),\n",
       "    'b_loc': array([6, 4]),\n",
       "    'h_loc': [array([4, 1])]},\n",
       "   'rm-state': array([0., 0.])}],\n",
       " -100,\n",
       " True,\n",
       " {'distance_B': 6.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test traveling to target 2 before target 1 (should terminate)\n",
    "\n",
    "#Reset environment \n",
    "test_env.reset()\n",
    "#Travel to second target\n",
    "test_env.step(0)\n",
    "test_env.step(0)\n",
    "test_env.step(1)\n",
    "test_env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cbe6027",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "the RM was set to a terminal state!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Test that can't take another step after terminal state reached \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/reward_evolution/reward_machines/reward_machines/rm_environment.py:93\u001b[0m, in \u001b[0;36mRewardMachineEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# update the RM state\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_u_id, rm_rew, rm_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_rm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_u_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_props\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# returning the result of this action\u001b[39;00m\n\u001b[1;32m     96\u001b[0m done \u001b[38;5;241m=\u001b[39m rm_done \u001b[38;5;129;01mor\u001b[39;00m env_done\n",
      "File \u001b[0;32m~/Desktop/reward_evolution/reward_machines/reward_machines/reward_machine.py:46\u001b[0m, in \u001b[0;36mRewardMachine.step\u001b[0;34m(self, u1, true_props, s_info, env_done)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03mEmulates an step on the reward machine from state *u1* when observing *true_props*.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mThe rest of the parameters are for computing the reward when working with non-simple RMs: s_info (extra state information to compute the reward).\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Computing the next state in the RM and checking if the episode is done\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m u1 \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminal_u, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe RM was set to a terminal state!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m u2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_next_state(u1, true_props)\n\u001b[1;32m     48\u001b[0m done \u001b[38;5;241m=\u001b[39m (u2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminal_u)\n",
      "\u001b[0;31mAssertionError\u001b[0m: the RM was set to a terminal state!"
     ]
    }
   ],
   "source": [
    "#Test that can't take another step after terminal state reached \n",
    "test_env.step(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RMEnv",
   "language": "python",
   "name": "rmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
